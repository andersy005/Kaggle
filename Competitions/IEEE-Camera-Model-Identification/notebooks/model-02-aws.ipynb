{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Copying-images-to-training,-validation,-and-test_directories\" data-toc-modified-id=\"Copying-images-to-training,-validation,-and-test_directories-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Copying images to training, validation, and test_directories</a></span></li></ul></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Preprocessing</a></span></li><li><span><a href=\"#Fitting-the-model-using-a-batch-generator\" data-toc-modified-id=\"Fitting-the-model-using-a-batch-generator-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Fitting the model using a batch generator</a></span></li><li><span><a href=\"#Saving-the-model\" data-toc-modified-id=\"Saving-the-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Saving the model</a></span></li><li><span><a href=\"#Displaying-curves-of-loss-and-accuracy-during-training\" data-toc-modified-id=\"Displaying-curves-of-loss-and-accuracy-during-training-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Displaying curves of loss and accuracy during training</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_dir = '/home/ubuntu/IEEE/train'\n",
    "train_dir = '/home/ubuntu/IEEE/camera_model_small/train'\n",
    "validation_dir = '/home/ubuntu/IEEE/camera_model_small/validation'\n",
    "test_dir = '/home/ubuntu/IEEE/camera_model_small/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HTC-1-M7',\n",
       " 'Motorola-X',\n",
       " 'iPhone-6',\n",
       " 'Motorola-Nexus-6',\n",
       " 'Samsung-Galaxy-Note3',\n",
       " 'iPhone-4s',\n",
       " 'Motorola-Droid-Maxx',\n",
       " 'Sony-NEX-7',\n",
       " 'LG-Nexus-5x',\n",
       " 'Samsung-Galaxy-S4']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get camera list\n",
    "cameras = os.listdir(original_dataset_dir)\n",
    "cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training HTC-1-M7 images: 275 \n",
      "Total training Motorola-X images: 275 \n",
      "Total training iPhone-6 images: 275 \n",
      "Total training Motorola-Nexus-6 images: 275 \n",
      "Total training Samsung-Galaxy-Note3 images: 275 \n",
      "Total training iPhone-4s images: 275 \n",
      "Total training Motorola-Droid-Maxx images: 275 \n",
      "Total training Sony-NEX-7 images: 275 \n",
      "Total training LG-Nexus-5x images: 275 \n",
      "Total training Samsung-Galaxy-S4 images: 275 \n"
     ]
    }
   ],
   "source": [
    "for camera in cameras:\n",
    "    camera_dir = os.path.join(original_dataset_dir, camera)\n",
    "    print('Total training {} images: {} '.format(camera, len(os.listdir(camera_dir))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training HTC-1-M7 images: 165 \n",
      "Total training Motorola-X images: 165 \n",
      "Total training iPhone-6 images: 165 \n",
      "Total training Motorola-Nexus-6 images: 165 \n",
      "Total training Samsung-Galaxy-Note3 images: 165 \n",
      "Total training iPhone-4s images: 165 \n",
      "Total training Motorola-Droid-Maxx images: 165 \n",
      "Total training Sony-NEX-7 images: 165 \n",
      "Total training LG-Nexus-5x images: 165 \n",
      "Total training Samsung-Galaxy-S4 images: 165 \n"
     ]
    }
   ],
   "source": [
    "for camera in cameras:\n",
    "    camera_dir = os.path.join(train_dir, camera)\n",
    "    print('Total training {} images: {} '.format(camera, len(os.listdir(camera_dir))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total validation HTC-1-M7 images: 55 \n",
      "Total validation Motorola-X images: 55 \n",
      "Total validation iPhone-6 images: 55 \n",
      "Total validation Motorola-Nexus-6 images: 55 \n",
      "Total validation Samsung-Galaxy-Note3 images: 55 \n",
      "Total validation iPhone-4s images: 55 \n",
      "Total validation Motorola-Droid-Maxx images: 55 \n",
      "Total validation Sony-NEX-7 images: 55 \n",
      "Total validation LG-Nexus-5x images: 55 \n",
      "Total validation Samsung-Galaxy-S4 images: 55 \n"
     ]
    }
   ],
   "source": [
    "for camera in cameras:\n",
    "    camera_dir = os.path.join(validation_dir, camera)\n",
    "    print('Total validation {} images: {} '.format(camera, len(os.listdir(camera_dir))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test HTC-1-M7 images: 55 \n",
      "Total test Motorola-X images: 55 \n",
      "Total test iPhone-6 images: 55 \n",
      "Total test Motorola-Nexus-6 images: 55 \n",
      "Total test Samsung-Galaxy-Note3 images: 55 \n",
      "Total test iPhone-4s images: 55 \n",
      "Total test Motorola-Droid-Maxx images: 55 \n",
      "Total test Sony-NEX-7 images: 55 \n",
      "Total test LG-Nexus-5x images: 55 \n",
      "Total test Samsung-Galaxy-S4 images: 55 \n"
     ]
    }
   ],
   "source": [
    "for camera in cameras:\n",
    "    camera_dir = os.path.join(test_dir, camera)\n",
    "    print('Total test {} images: {} '.format(camera, len(os.listdir(camera_dir))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a pretrained convnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers \n",
    "from keras import models\n",
    "from keras.applications import InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_dim=(224, 224, 3)):\n",
    "    input_tensor = layers.Input(shape=img_dim)\n",
    "    conv_base = InceptionResNetV2(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=img_dim)\n",
    "    conv_base.trainable = True\n",
    "    set_trainable = False\n",
    "    for layer in conv_base.layers:\n",
    "        if layer.name == 'block8_10_mixed':\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "    \n",
    "        else:\n",
    "            layer.trainable = False\n",
    "        \n",
    "    bn = layers.normalization.BatchNormalization()(input_tensor)\n",
    "    x = conv_base(bn)\n",
    "    x = layers.Flatten()(x)\n",
    "    output = layers.Dense(10, activation='softmax')(x)\n",
    "    model = models.Model(input_tensor, output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 4s 0us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_204 (Bat (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_resnet_v2 (Model)  (None, 5, 5, 1536)        54336736  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                384010    \n",
      "=================================================================\n",
      "Total params: 54,720,758\n",
      "Trainable params: 4,514,352\n",
      "Non-trainable params: 50,206,406\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Currently, the data sits on a drive as JPEG files, so the steps for getting it into the network are roughly as follows:\n",
    "\n",
    "    Read the picture files\n",
    "    Decode the JPEG content to RGB grids of pixels\n",
    "    Convert these into floating point tensors\n",
    "    Rescale the pixel values to the [0-1] range\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1650 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 550 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape:  (16, 224, 224, 3)\n",
      "labels batch shape:  (16, 10)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape: ', data_batch.shape)\n",
    "    print('labels batch shape: ', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model using a batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 99/100 [============================>.] - ETA: 3s - loss: 2.2805 - acc: 0.2367"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=100,\n",
    "        epochs = 30,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps = 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying curves of loss and accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    history_dict = history.history\n",
    "    loss_values = history_dict['loss']\n",
    "    val_loss_values = history_dict['val_loss']\n",
    "    acc_values = history_dict['acc']\n",
    "    val_acc_values = history_dict['val_acc']\n",
    "    epochs = range(1, len(acc_values) + 1)\n",
    "    \n",
    "    plt.plot(epochs, loss_values, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss_values, 'r', label='Validation loss')\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(epochs, acc_values, 'b', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc_values, 'r', label='Validation accuracy')\n",
    "    plt.title('Training and Validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Validation')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
